\documentclass{../../tex_import/ETHuebung_english}

\usepackage{../../tex_import/exercise_ml}

\input{../../tex_import/definitions} %our customized .tex macros

\begin{document}


\makeheader{11, Nov 30, 2017}{Theory Questions, PyTorch Introduction}


\ProblemV{1}{How to compute $\vU$ and $\vS$ efficiently}{
Consider the $N \times N$ matrix $\vX^\top \vX$. Similar to before we have
\begin{flalign*}
\vX^\top \vX &= \vV \vS^\top \vS \vV^\top.
\end{flalign*}
Let $\vv_i$, $i=1, \cdots, D$, denote the columns of $\vV$. Then
\begin{align} \label{equ:eigenvalue}
\vX^\top \vX \vv_j&=  \vV \vS^\top \vS \vV^\top \vv_j = s_j^2 \vv_j.
\end{align}
So we see that the $j$-th column of $\vV$ is an eigenvector of
$\vX^\top \vX$ with eigenvalue $s_j^2$.  Therefore, solving the
eigenvector/value problem for the matrix $\vX^\top \vX$ gives us a
way to compute $\vV$ and $\vS$.

Now multiply the identity (\ref{equ:eigenvalue}) from the left by the matrix $\vX$. We get
\begin{align*}
\vX \vX^\top (\vX \vv_j) &= s_j^2 (\vX \vv_j).
\end{align*}
We see therefore that $\uv_j = \vX \vv_j$ and so we can compute the
desired eigenvectors $\uv_j$ from the eigenvectors $\vv_j$ without
having to solve the $D \times D$ eigenvector/value problem.
}

\ProblemV{2}{Positive semi-definite}{
Look at $\vA=\vX \vX^\top$  and $B=\vX^\top \vX$. By the SVD we know that
$\vX=\vU \vS \vV^\top$.  As we discussed in the course, the columns
of $\vU$ are eigenvectors of the first matrix and the columns of
$\vV$ are eigenvectors of the second matrix.  But note that $\vA=\vB$
since $\vX$ is symmetric.  Hence the eigenspace associated to each
distinct eigenvalue of $\vA$ is equal to the eigenspace associated
to the same eigenvalue of $\vB$.

Set $\vU=\vV$ and let the columns of $\vU$ be eigenvectors of $\vA$.
Compute $\vU^\top \vX \vV$. This gives us a diagonal matrix which we
can define to be $\vS$.  It's entries are not necessarily non-negative.

If the matrix is in addition positive semi-definite then the diagonal
entries of $\vS$ must in fact must be non-negative -- multiplying
the matrix from the left by $\uv_j^\top$ and from the right by $\uv_j$
gives $s_j$ which must be non-negative if the quadratic form given
by the matrix is assumed to be positive-definite.  }

\end{document}
